nest[i,] <- simple.bounds(s, lb, ub)
}
return(nest)
}
cuckoos.best.nest <- function(nest, newnest, fitness) {
for (i in 1:dim(nest)[1]) {
fnew <- fobj(newnest[i,])
if (fnew <= fitness[i]) {
fitness[i] <- fnew
nest[i,] <- newnest[i,]
}
}
# Find the current best
fmin <- min(fitness)
best <- nest[which.min(fitness)]
return(list('fmin' = fmin, 'best' = best, 'nest' = nest, 'fitness' = fitness))
}
## Replace some nests by constructing new solutions/nests
empty.nests <- function(nest, lb, ub, pa) {
# A fraction of worse nests are discovered with a probability pa
n <- dim(nest)[1]
o <- dim(nest)[2]
# Discovery or not -- a status vector
k <- matrix(runif(n*o), n, o)>pa
# In the real world,  if a cuckoo's egg is very similar to host's eggs, then
# this cuckoo's egg is less likely to be discovered, thus the fitness should
# be related to the difference in solutions. Therefore, it is a good idea
# to do a random walk in a biased way with some random step sizes.
## New solution by biased/selective random walks
stepsize <- runif(1)*(nest[sample.int(n),]-nest[sample.int(n),])
return(nest*stepsize*k)
}
# Application of simple constraints
simple.bounds <- function(s, lb, ub) {
# Apply the lower bound
ns_tmp <- s
i <- ns_tmp<lb
ns_tmp[i] <- lb[i]
# Apply the upper bounds
j <- ns_tmp>ub
ns_tmp[j] = ub[j]
# Update this new move
return(ns_tmp)
}
# You can replace the following by your own functions
# A d-dimensional objective functions
fobj <- function(u) {
## d-dimensional sphere function sum_j=1^d (u_j-1)^2.
# with a minimum at (1,1, ...., 1);
return(sum((u-1)^2));
}
cuckoo.search()
cuckoo.search <- function(n = 25) {
# discovery rate of alien eggs/solutions
pa <- 0.25
## Change this if you want to get better results
# Tolerance
tol <- 1.0e-5
## Simple bounds of the search domain
nd <- 15
lb <- matrix(-5, 1, nd)
ub <- matrix(5, 1, nd)
# Random initial solutions
nest <- matrix(0, n, nd)
for (i in 1:n) {
nest[i,] <- lb + (ub - lb) * runif(nd)
}
# Get the current best
fitness <- 10^10 * matrix(1, n, 1)
current <- cuckoo.best.nest(nest, nest, fitness)
fmin <- current$fmin
bestnest <- current$best
nest <- current$nest
fitness <- current$fitness
iter <- 0
# Start iterations
while (fmin > tol) {
# Generate new solutions (but keep the current best)
new_nest <- cuckoos.nest(nest, bestnest, lb, ub)
new_best <- cuckoos.best.nest(nest, new_nest, fitness)
fnew <- new_best$fmin
best <- new_best$best
nest <- new_best$nest
fitness <- new_best$fitness
# Update the counter
iter <- iter + n
# Discovery and randomization
new_nest <- empty.nests(nest, lb, ub, pa)
# Evaluate this set of solutions
new_best <- cuckoos.best.nest(nest, new_nest, fitness)
fnew <- new_best$fmin
best <- new_best$best
nest <- new_best$nest
fitness <- new_best$fitness
iter <- iter + n
# find the best objective so far
if (fnew<fmin) {
fmin <- fnew
bestnest <- best
}
print(cat('iter:',iter, 'fitness:', fmin))
}
# Post optimization and processing
print(cat('Total number of iterations=', iter))
return(list('fmin' = fmin, 'bestnest' = bestnest))
}
# Get cuckoos by random walk
cuckoos.nest <- function(nest, best, lb, ub) {
# Levy flights
n <- dim(nest)[1]
# Levy exponent and coefficient
# For details, see equation (2.21), Page 16 (chapter 2) of the book
# X. S. Yang, Nature-Inspired Metaheuristic Algorithms, 2nd Edition, Luniver Press, (2010).
beta <- 3/2
sigma <- (gamma(1+beta)*sin(pi*beta/2)/(gamma((1+beta)/2)*beta*2^((beta-1)/2)))^(1/beta)
for (i in 1:n) {
s <- nest[i,]
size <- dim(nest)[2]
# This is a simple way of implementing Levy flights
# For standard random walks, use step = 1
## Levy flights by Mantegnas's algorithm
u <- rnorm(size)*sigma
v <- rnorm(size)
step <- u/abs(v)^(1/beta)
# In the next equation, the difference factor (s-best) means that
# When the solution is the best solution, it remains unchanged.
stepsize <- 0.01*step*(s-best)
# Here the factor 0.01 comes from the fact that L/100 should be typical
# step size of walks/flights where L is the typical lenghtscale;
# otherwise, Levy flights may become too aggresive/efficient.
# which makes new solutions (even) jump out side of the design domain
# (and thus wasting evaluations).
# Now the actual random walks or flights
s <- s+stepsize*rnorm(size)
# Apply simple bounds/limits
nest[i,] <- simple.bounds(s, lb, ub)
}
return(nest)
}
cuckoos.best.nest <- function(nest, newnest, fitness) {
for (i in 1:dim(nest)[1]) {
fnew <- fobj(newnest[i,])
if (fnew <= fitness[i]) {
fitness[i] <- fnew
nest[i,] <- newnest[i,]
}
}
# Find the current best
fmin <- min(fitness)
best <- nest[which.min(fitness)]
return(list('fmin' = fmin, 'best' = best, 'nest' = nest, 'fitness' = fitness))
}
## Replace some nests by constructing new solutions/nests
empty.nests <- function(nest, lb, ub, pa) {
# A fraction of worse nests are discovered with a probability pa
n <- dim(nest)[1]
o <- dim(nest)[2]
# Discovery or not -- a status vector
k <- matrix(runif(n*o), n, o)>pa
# In the real world,  if a cuckoo's egg is very similar to host's eggs, then
# this cuckoo's egg is less likely to be discovered, thus the fitness should
# be related to the difference in solutions. Therefore, it is a good idea
# to do a random walk in a biased way with some random step sizes.
## New solution by biased/selective random walks
stepsize <- runif(1)*(nest[sample.int(n),]-nest[sample.int(n),])
return(nest*stepsize*k)
}
# Application of simple constraints
simple.bounds <- function(s, lb, ub) {
# Apply the lower bound
ns_tmp <- s
i <- ns_tmp<lb
ns_tmp[i] <- lb[i]
# Apply the upper bounds
j <- ns_tmp>ub
ns_tmp[j] = ub[j]
# Update this new move
return(ns_tmp)
}
# You can replace the following by your own functions
# A d-dimensional objective functions
fobj <- function(u) {
## d-dimensional sphere function sum_j=1^d (u_j-1)^2.
# with a minimum at (1,1, ...., 1);
return(sum((u-1)^2));
}
cuckoo.search(45)
cuckoo.search <- function(n = 25) {
# discovery rate of alien eggs/solutions
pa <- 0.25
## Change this if you want to get better results
# Tolerance
tol <- 1.0e-5
## Simple bounds of the search domain
nd <- 15
lb <- matrix(-5, 1, nd)
ub <- matrix(5, 1, nd)
# Random initial solutions
nest <- matrix(0, n, nd)
for (i in 1:n) {
nest[i,] <- lb + (ub - lb) * runif(nd)
}
# Get the current best
fitness <- 10^10 * matrix(1, n, 1)
current <- cuckoo.best.nest(nest, nest, fitness)
fmin <- current$fmin
bestnest <- current$best
nest <- current$nest
fitness <- current$fitness
iter <- 0
# Start iterations
while (fmin > tol) {
# Generate new solutions (but keep the current best)
new_nest <- cuckoos.nest(nest, bestnest, lb, ub)
new_best <- cuckoos.best.nest(nest, new_nest, fitness)
fnew <- new_best$fmin
best <- new_best$best
nest <- new_best$nest
fitness <- new_best$fitness
# Update the counter
iter <- iter + n
# Discovery and randomization
new_nest <- empty.nests(nest, lb, ub, pa)
# Evaluate this set of solutions
new_best <- cuckoos.best.nest(nest, new_nest, fitness)
fnew <- new_best$fmin
best <- new_best$best
nest <- new_best$nest
fitness <- new_best$fitness
iter <- iter + n
# find the best objective so far
if (fnew<fmin) {
fmin <- fnew
bestnest <- best
}
print(cat('iter:',iter, 'fitness:', fmin))
}
# Post optimization and processing
print(cat('Total number of iterations=', iter))
return(list('fmin' = fmin, 'bestnest' = bestnest))
}
# Get cuckoos by random walk
cuckoos.nest <- function(nest, best, lb, ub) {
# Levy flights
n <- dim(nest)[1]
# Levy exponent and coefficient
# For details, see equation (2.21), Page 16 (chapter 2) of the book
# X. S. Yang, Nature-Inspired Metaheuristic Algorithms, 2nd Edition, Luniver Press, (2010).
beta <- 3/2
sigma <- (gamma(1+beta)*sin(pi*beta/2)/(gamma((1+beta)/2)*beta*2^((beta-1)/2)))^(1/beta)
for (i in 1:n) {
s <- nest[i,]
size <- dim(nest)[2]
# This is a simple way of implementing Levy flights
# For standard random walks, use step = 1
## Levy flights by Mantegnas's algorithm
u <- rnorm(size)*sigma
v <- rnorm(size)
step <- u/abs(v)^(1/beta)
# In the next equation, the difference factor (s-best) means that
# When the solution is the best solution, it remains unchanged.
stepsize <- 0.01*step*(s-best)
# Here the factor 0.01 comes from the fact that L/100 should be typical
# step size of walks/flights where L is the typical lenghtscale;
# otherwise, Levy flights may become too aggresive/efficient.
# which makes new solutions (even) jump out side of the design domain
# (and thus wasting evaluations).
# Now the actual random walks or flights
s <- s+stepsize*rnorm(size)
# Apply simple bounds/limits
nest[i,] <- simple.bounds(s, lb, ub)
}
return(nest)
}
cuckoos.best.nest <- function(nest, newnest, fitness) {
for (i in 1:dim(nest)[1]) {
fnew <- fobj(newnest[i,])
if (fnew <= fitness[i]) {
fitness[i] <- fnew
nest[i,] <- newnest[i,]
}
}
# Find the current best
fmin <- min(fitness)
best <- nest[which.min(fitness)]
return(list('fmin' = fmin, 'best' = best, 'nest' = nest, 'fitness' = fitness))
}
## Replace some nests by constructing new solutions/nests
empty.nests <- function(nest, lb, ub, pa) {
# A fraction of worse nests are discovered with a probability pa
n <- dim(nest)[1]
o <- dim(nest)[2]
# Discovery or not -- a status vector
k <- matrix(runif(n*o), n, o)>pa
# In the real world,  if a cuckoo's egg is very similar to host's eggs, then
# this cuckoo's egg is less likely to be discovered, thus the fitness should
# be related to the difference in solutions. Therefore, it is a good idea
# to do a random walk in a biased way with some random step sizes.
## New solution by biased/selective random walks
stepsize <- runif(1)*(nest[sample.int(n),]-nest[sample.int(n),])
return(nest*stepsize*k)
}
# Application of simple constraints
simple.bounds <- function(s, lb, ub) {
# Apply the lower bound
ns_tmp <- s
i <- ns_tmp<lb
ns_tmp[i] <- lb[i]
# Apply the upper bounds
j <- ns_tmp>ub
ns_tmp[j] = ub[j]
# Update this new move
return(ns_tmp)
}
# You can replace the following by your own functions
# A d-dimensional objective functions
fobj <- function(u) {
## d-dimensional sphere function sum_j=1^d (u_j-1)^2.
# with a minimum at (1,1, ...., 1);
return(sum((u-1)^2));
}
levy <- function(xx)
{
##########################################################################
#
# LEVY FUNCTION
#
# Authors: Sonja Surjanovic, Simon Fraser University
#          Derek Bingham, Simon Fraser University
# Questions/Comments: Please email Derek Bingham at dbingham@stat.sfu.ca.
#
# Copyright 2013. Derek Bingham, Simon Fraser University.
#
# THERE IS NO WARRANTY, EXPRESS OR IMPLIED. WE DO NOT ASSUME ANY LIABILITY
# FOR THE USE OF THIS SOFTWARE.  If software is modified to produce
# derivative works, such modified software should be clearly marked.
# Additionally, this program is free software; you can redistribute it
# and/or modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; version 2.0 of the License.
# Accordingly, this program is distributed in the hope that it will be
# useful, but WITHOUT ANY WARRANTY; without even the implied warranty
# of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# For function details and reference information, see:
# http://www.sfu.ca/~ssurjano/
#
##########################################################################
#
# INPUT:
#
# xx = c(x1, x2, ..., xd)
#
##########################################################################
d <- length(xx)
w <- 1 + (xx - 1)/4
term1 <- (sin(pi*w[1]))^2
term3 <- (w[d]-1)^2 * (1+1*(sin(2*pi*w[d]))^2)
wi <- w[1:(d-1)]
sum <- sum((wi-1)^2 * (1+10*(sin(pi*wi+1))^2))
y <- term1 + sum + term3
return(y)
}
psoptim <- function(FUN, n=100, max.loop=100, w=.9, c1=.2, c2=.2, xmin, xmax, vmax=c(4,4), seed=10, anim=TRUE)
{
#FUN - nazwa optymalizowanej funkcji, funkcja musi byc jednoargumentowa
#co oznacza ze argumentem jest wektor
# n - liczba czastek w roju
# max.loop - maksymalna liczba iteracji
# w - wspolczynnik bezwladnosci
# c1 - wspolczynnik wlasnego "zaufania"
# c2 - wspolczynnik "zaufania" do roju
# xmin - wektor okreslajacy dolne ograniczenia wartosci zmiennych
# xmax - wektor okreslajacy gorne ograniczenia wartosci zmiennych
# vmax - wektor ograniczen predkosci w kazdym kierunku
# seed - liczba naturalna okreslajaca tzw. ziarno dla generatora liczb
#pseudolosowych argument stosowany w celu uzyskania powtarzalnosci otrzymywanych wynikow
# anim - wartosc logiczna, jesli prawda TRUE(wartosc domyslna), to animacja przebiegu
# optymalizajci jest wlaczona
g1 <- function(x,y)
{
FUN(cbind(x,y))
}
d <- length(xmin)
set.seed(seed)
x <- matrix(nrow=n, ncol=d)
for(i in 1:d)
x[,i] <- runif(n, xmin[1],xmax[1])
wart.f <- FUN(x)
x.best.czastki <- x
x.best.roju <- matrix(x[which.max(wart.f),], ncol=d)
if((d == 2) && anim)
{
x_image <- matrix(c(seq(from=xmin[1],to=xmax[1],length.out=100),seq(from=xmin[2],to=xmax[2],length.out=100)), ncol=2)
z <- outer(x_image[,1], x_image[,2], g1)
image(x_image[,1], x_image[,2], z, xlab="x1", ylab="x2", main="Initial swarm")
contour(x_image[,1], x_image[,2], z, nlevels=10, add=TRUE, col="grey50")
points(x[,1], x[,2], pch=19, col="darkslateblue")
}
if (interactive() && anim && (d==2)) {
invisible(readline(prompt = "Press <Enter>/<Return> to continue..."))
}
v <- matrix(runif(n*d, min=-vmax, max=vmax), ncol=d, nrow=n)
g.mean <- c()
g.best <- c()
loop <- 1
while(loop <= max.loop)
{
wart.f <- FUN(x)
g.mean <- rbind(g.mean, mean(wart.f))
idx <- which(wart.f > FUN(x.best.czastki))
x.best.czastki[idx,] <- x[idx,]
x.best.roju.nowe <- matrix(x[which.max(FUN(x.best.czastki)),],ncol=d)
if(FUN(x.best.roju.nowe) > FUN(x.best.roju))
x.best.roju <- x.best.roju.nowe
g.best <- rbind(g.best, FUN(x.best.roju))
for(i in 1:n)
{
for(j in 1:d)
{
r1 <- runif(1)
r2 <- runif(1)
v[i,j] <- w*v[i,j] + c1*r1*(x.best.czastki[i,j] - x[i,j]) +  c2*r2*(x.best.roju[j]-x[i,j])
if(v[i,j] > vmax[j] || v[i,j] < -vmax[j])
v[i,j] <- vmax[j]
x_prev <- x[i,j]
x[i,j] <- x[i,j] + v[i,j]
if(x[i,j] > xmax[j])
x[i,j] <- x_prev
if(x[i,j] < xmin[j])
x[i,j] <- x_prev
}
}
if((d==2) && anim)
{
contour(x_image[,1], x_image[,2], z, nlevels=20, xlab="x1", ylab="x2", col="darkgray", main=paste(loop, "/", max.loop))
points(x, xlim=c(xmin[1], xmax[1]), ylim=c(xmin[2], xmax[2]), pch=21, bg="cadetblue")
# points(x, xlim=c(xmin[1], xmax[1]), ylim=c(xmin[2], xmax[2]), pch=21, bg=densCols(x))
}
loop <- loop + 1
}
if(anim)
{
if (interactive() & (d==2)) {
invisible(readline(prompt = "Press <Enter>/<Return> to continue..."))
}
}
plot(g.best, type="o", col="darkgreen", pch=19, cex=.7, ylim=c(min(g.mean),max(g.best)), xlab="Iteration", ylab="Fitness value")
lines(g.mean, type="o", col="blue", cex=.7, pch=19)
legend("bottomright", legend = c("Best", "Mean"), col = c("darkgreen", "blue"), pch = 19, lty = 1, merge = TRUE)
colnames(x.best.roju) <- paste("x", seq(1:d), sep="")
res <- list(sol = x.best.roju, val=g.best[loop-1])
return(res)
}
n <- 50
m.l <- 50
w <- 0.95
c1 <- 0.2
c2 <- 0.2
xmin <- c(-5.12, -5.12)
xmax <- c(5.12, 5.12)
vmax <- c(4, 4)
start.time <- Sys.time()
psoptim(FUN=levy, n=n, max.loop=m.l, w=w, c1=c1, c2=c2,
xmin=xmin, xmax=xmax, vmax=vmax, seed=5, anim=FALSE)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
n <- 50
m.l <- 1000
w <- 0.95
c1 <- 0.2
c2 <- 0.2
xmin <- c(-5.12, -5.12)
xmax <- c(5.12, 5.12)
vmax <- c(4, 4)
start.time <- Sys.time()
psoptim(FUN=levy, n=n, max.loop=m.l, w=w, c1=c1, c2=c2,
xmin=xmin, xmax=xmax, vmax=vmax, seed=5, anim=FALSE)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
n <- 50
m.l <- 50
w <- 0.95
c1 <- 0.2
c2 <- 0.2
xmin <- c(-10.12, -10.12)
xmax <- c(5.12, 5.12)
vmax <- c(4, 4)
start.time <- Sys.time()
psoptim(FUN=levy, n=n, max.loop=m.l, w=w, c1=c1, c2=c2,
xmin=xmin, xmax=xmax, vmax=vmax, seed=5, anim=FALSE)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
library(microbats)
# find the x-value that gives the minimum of the quadratic function y = x^2 - 3x
# x should then be 1.5
quad_func <- function(D, sol) {
val = 0
for (i in 1:D) {
val <- val + sol[i] * sol[i] - sol[i] * 3
}
return(val)
}
# run a simulation using the standard bat algorithm
set.seed(5) # for reproducive results
fit <- bat_optim(D = 1, NP = 20, N_Gen = 100, A = 0.5, r = 0.5,
Qmin = 0, Qmax = 2, Lower = -10, Upper = 10, FUN = quad_func)
x <- fit$best_solution
remove.packages("microbats", lib="~/R/win-library/3.4")
